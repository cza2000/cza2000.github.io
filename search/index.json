[{"content":"Lab3: KVRaft lab链接 https://pdos.csail.mit.edu/6.824/labs/lab-kvraft.html\n本次lab中我们需要使用lab2中实现的Raft库来构建一个可容错的 Key/Value 存储服务，要求其对外提供强一致性（Strong consistency）。\n这个KV存储服务支持Get/Put/Append三种客户端操作。客户端通过RPC与集群中的leader通信，leader接收到请求后将其包装在一条Raft日志中下放到Raft层进行共识，日志被apply后返回客户端结果。\n一些思考 在PartA的描述中提到，leader在将一个请求下放到raft层之后，commit之前宕机，这时它无法回复Clerk。又或者是，这条日志成功commit，但返回的RPC丢失。Clerk在规定时间内没有收到结果，会向另一台主机（可能是新选出的leader）发送RPC，这条日志最终被commit之后又会被应用于状态机，从而状态机执行了两次相同的请求。\n这要求我们能够判断重复的请求。因此每个请求都需要被唯一标识，请求中需要加上（ClientID, RequestID），Clerk每次请求成功之后自增RequestID。\n我们还需要在遇到重复的请求时直接返回第一次请求时的结果，这需要我们保存每一个Clerk的最后一次请求的结果ClientID -\u0026gt; (RequestID, LastResponse)。只需要保存最后一次请求结果是因为如果服务端收到RequestID = x的RPC，说明这个Clerk已经收到了RequestID为[1, x-1]之间内的所有请求的结果，服务端如果再次收到这个RequestID在区间之内的请求说明该RPC过期，直接丢弃即可。\nclient 我将3种请求共用了一个RPC，简化了逻辑。\nClerk保存一个leaderID，请求失败了再换另一个server，请求成功了自增requestID。\ntype Clerk struct { servers []*labrpc.ClientEnd // You will have to modify this struct. \tleaderID int clientID int64 requestID int } func (ck *Clerk) Get(key string) string { return ck.Command(key, \u0026#34;\u0026#34;, OpGet) } func (ck *Clerk) Put(key string, value string) { ck.Command(key, value, OpPut) } func (ck *Clerk) Append(key string, value string) { ck.Command(key, value, OpAppend) } func (ck *Clerk) Command(key, value string, op Operation) string { req := getCommandRequest(key, value, op, int(ck.clientID), ck.requestID)\tfor { resp := CommandResponse{} if !ck.servers[ck.leaderID].Call(\u0026#34;KVServer.Command\u0026#34;, \u0026amp;req, \u0026amp;resp) || resp.Err == ErrWrongLeader || resp.Err == ErrTimeout {\tck.leaderID = (ck.leaderID + 1) % len(ck.servers) continue } ck.requestID++ return\tresp.Value\t} } server KVServer的结构体如下：\ntype KVServer struct { mu sync.RWMutex me int rf *raft.Raft applyCh chan raft.ApplyMsg dead int32 // set by Kill()  maxraftstate int // snapshot if log grows this big  // Your definitions here. \tpersister *raft.Persister notifyChans map[int]chan *CommandResponse db map[string]string lastSessions map[int]Session lastAppliedIndex int } state-machine 本次lab中只需使用一个内存版本的 KV 状态机 map[string]string。\nRPC handler 客户端请求来临时，Server端会启动一个协程作为RPC handler来处理客户端请求，其中会调用 Raft.Start 函数将请求下放到Raft层形成一条日志去做共识。在Raft层，每条被commit的日志会按照index的顺序写入applyCh中，上层必须也按index序从applyCh中读出日志并应用于状态机，这样才能保证不同节点上的数据一致。\n这要求必须有一个单独的applier协程来循环读applyCh，并应用于状态机。由于来自不同客户端的请求是并发的，如果在RPC handler协程中直接读applyCh无法保证index序。返回给客户端的response要根据日志应用于状态机的结果来生成，这需要我们处理RPC handler和applier协程之间的通信问题。\n自然想到使用channel来通信，使用一个 notifyChans map (log index -\u0026gt; response) 来记录每一个请求对应的channel。在RPC handler协程将日志下放到Raft层之后，在notifyChans中注册一个channel并阻塞读，applier协程读出日志，应用于状态机之后生成response写入这个channel。RPC handler协程在规定时间内读出结果则正常返回客户端，若超时则返回超时。\nfunc (kv *KVServer) Command(req *CommandRequest, resp *CommandResponse) { // Your code here. \tkv.mu.RLock() defer DPrintf(\u0026#34;[KVServer %d] reply %+v for Request %+v\u0026#34;, kv.me, resp, req) DPrintf(\u0026#34;[KVServer %d] received Request %+v from Clerk\u0026#34;, kv.me, req) if req.Op != OpGet \u0026amp;\u0026amp; kv.isDuplicatedRequest(req.ClientID, req.RequestID) { resp.Err = kv.lastSessions[req.ClientID].Err kv.mu.RUnlock() return } kv.mu.RUnlock() index, _, isLeader := kv.rf.Start(*req) if !isLeader { resp.Err = ErrWrongLeader return } DPrintf(\u0026#34;[KVServer %d] add command into raft layer [index %d]\u0026#34;, kv.me, index) kv.mu.Lock() ch := kv.getNotifyChan(index) kv.mu.Unlock() select { case result := \u0026lt;-ch: resp.Value = result.Value resp.Err = result.Err case \u0026lt;-time.NewTimer(500 * time.Millisecond).C: resp.Err = ErrTimeout } go func() { kv.mu.Lock() kv.removeNotifyChan(index) kv.mu.Unlock() }() } func (kv *KVServer) applier() { for !kv.killed() { select { case applyMsg := \u0026lt;-kv.applyCh: if applyMsg.CommandValid { command := applyMsg.Command.(CommandRequest) kv.mu.Lock() if applyMsg.CommandIndex \u0026lt;= kv.lastAppliedIndex { DPrintf(\u0026#34;[KVServer %d] discard out-of-date apply Msg in [index %d]\u0026#34;, kv.me, applyMsg.CommandIndex) kv.mu.Unlock() continue } kv.lastAppliedIndex = applyMsg.CommandIndex var response *CommandResponse if command.Op != OpGet \u0026amp;\u0026amp; kv.isDuplicatedRequest(command.ClientID, command.RequestID) { DPrintf(\u0026#34;[KVServer %d] received a duplicated command in [index %d]\u0026#34;, kv.me, applyMsg.CommandIndex) response = kv.lastSessions[command.ClientID].CommandResponse } else { response = kv.applyCommand(command) if command.Op != OpGet { kv.lastSessions[command.ClientID] = Session{ RequestID: command.RequestID, CommandResponse: response, } } } if kv.needToSnapshot(applyMsg.RaftStateSize) { DPrintf(\u0026#34;[KVServer %d] reach maxraftstate, take a snapshot till [index %d]\u0026#34;, kv.me, applyMsg.CommandIndex) kv.takeSnapshot(applyMsg.CommandIndex) } if currentTerm, isLeader := kv.rf.GetState(); currentTerm == applyMsg.CommandTerm \u0026amp;\u0026amp; isLeader { ch := kv.getNotifyChan(applyMsg.CommandIndex) ch \u0026lt;- response } kv.mu.Unlock() } else { kv.mu.Lock() DPrintf(\u0026#34;[KVServer %d] received a snapshot from raft layer [index %d, term %d]\u0026#34;, kv.me, applyMsg.SnapshotIndex, applyMsg.SnapshotTerm) if kv.rf.CondInstallSnapshot(applyMsg.SnapshotTerm, applyMsg.SnapshotIndex, applyMsg.Snapshot) { kv.applySnapshotToService(applyMsg.Snapshot) kv.lastAppliedIndex = applyMsg.SnapshotIndex } kv.mu.Unlock() } } } } 有几点需要注意：\n  apply日志时需要防止状态机回滚。在lab2中提到作为follower的节点可能收到leader的install snapshot，将snapshot写入applyCh中，此时读applyCh的顺序是：旧日志1 -\u0026gt; 新快照 -\u0026gt; 旧日志2。应用了新快照之后要避免再次应用旧日志，所以应用快照之后也要更新 lastAppliedIndex，应用日志时要先判断是否 applyMsg.CommandIndex \u0026lt;= kv.lastAppliedIndex。\n  仅对leader的notifyChan进行通知。每个节点在读出日志后都要提交到状态机，且更新lastSessions。但只有leader需要将response写入notifyChan。leader可能会在提交日志后失去leader身份，所以在applier中写入response前要先判断。此时RPC handler协程就让其超时。\n  客户端的非读请求需要两次去重。重复的请求到来时，之前相同的请求可能已经被应用于该节点的状态机，也可能其对应的日志还没被commit。因此需要在RPC handler中调用Start之前以及日志commit之后应用于状态机之前两次去重。\n  leader在调用Start提交日志后去获取notifyChan来阻塞读 以及 applier 在commit日志并应用于状态机之后获取notifyChan来写入response 这二者之间顺序无法保证。因此channel容量设置为1，先获取channel的协程要负责创建channel，这个过程要加写锁。\n  func (kv *KVServer) getNotifyChan(index int) chan *CommandResponse { ch, ok := kv.notifyChans[index] if !ok { ch := make(chan *CommandResponse, 1) kv.notifyChans[index] = ch return ch } return ch } snapshot part B中要求我们在raft state size达到阈值时给raft层下发快照。快照中不仅需要包含KV状态机，还需要包含lastSessions客户端请求去重表。由于快照是和lastIncludeIndex对应的，所以需要由applier协程在将对应的index的日志应用于状态机后继续阻塞的生成快照。\n","date":"2022-02-03T15:06:21+08:00","permalink":"https://cza2000.github.io/2022/mit-6.824-spring-2021-lab3-raftkv/","title":"mit-6.824 Spring 2021 lab3: RaftKV"},{"content":"Lab2: Raft lab原链接 https://pdos.csail.mit.edu/6.824/labs/lab-raft.html\nRaft是一种基于复制的状态机协议，通过在多个副本服务器上存储其状态（即数据）的完整副本来实现容错。\nRaft将客户端请求组织成一个称为日志的序列，并通过复制确保所有副本服务器都看到相同的日志。每个副本按日志顺序执行客户端请求，并将它们应用于本地的状态机副本。由于所有副本服务器都看到相同的日志内容，因此它们都以相同的顺序执行相同的请求，从而继续具有相同的服务状态。如果服务器出现故障但随后恢复，Raft保证只要至少半数的服务器存活，并且可以相互通信，就可以保证正常对外服务。\n在本次lab中我们的任务是使用go语言实现raft。参考论文 raft-extended，我们需要实现除了集群成员变更之外的绝大部分内容。论文中我认为最核心的就是描述3个RPC的(Figure 2)这张图，我的实现大体上遵循了这张图。此外我也参考了一些工业级的raft实现，比如SOFAJraft、etcd，做了一些优化。在我秋招面试美团的一个做分布式存储的部门时，他们问了我很多关于raft的内容（虽然最后挂了）。\n有些需要注意的点：\n 当收到的RPC中的term大于自身时，无条件跟随term并转为follower，这在不同的RPC handler中的处理略有不同。 在lab的一些测试用例中，网络将是不稳定的，带来大量随机的RPC丢包、乱序、超时。对于过期的RPC，直接抛弃不处理即可。对于是否过期的判断体现在term太小、身份不正确之类（例如follow收到append entries response）。 锁的使用：在接发RPC、读写channel时一定不要持有锁，不然很有可能死锁。此外有许多代码块对Raft结构中各字段是只读的，我使用了读写锁。  结构体 Raft结构中的各个变量和论文大致一样。\ntype Raft struct { rw sync.RWMutex // Lock to protect shared access to this peer\u0026#39;s state \tpeers []*labrpc.ClientEnd // RPC end points of all peers \tpersister *Persister // Object to hold this peer\u0026#39;s persisted state \tme int // this peer\u0026#39;s index into peers[] \tdead int32 // set by Kill() \t// Your data here (2A, 2B, 2C). \t// Look at the paper\u0026#39;s Figure 2 for a description of what \t// state a Raft server must maintain.  currentState State currentTerm int votedFor int voteFrom map[int]bool logs []LogEntry commitIndex int lastApplied int nextIndex []int matchIndex []int electionTimer *time.Timer heartbeatTimer *time.Timer applyCh chan ApplyMsg applierCond sync.Cond replicatorCond []sync.Cond } func Make(peers []*labrpc.ClientEnd, me int, persister *Persister, applyCh chan ApplyMsg) *Raft { // Your initialization code here (2A, 2B, 2C). \trf := \u0026amp;Raft{ rw: sync.RWMutex{}, peers: peers, persister: persister, me: me, dead: -1, currentState: Follower, currentTerm: 0, votedFor: -1, voteFrom: make(map[int]bool), logs: make([]LogEntry, 1), nextIndex: make([]int, len(peers)), matchIndex: make([]int, len(peers)), electionTimer: time.NewTimer(RandomizedElectionTimeout()), heartbeatTimer: time.NewTimer(StableHeartbeatTimeout()), applyCh: applyCh, replicatorCond: make([]sync.Cond, len(peers)), } rf.applierCond = *sync.NewCond(\u0026amp;rf.rw) rf.logs[0] = LogEntry{0, 0, nil} // initialize from state persisted before a crash \trf.readPersist(persister.ReadRaftState()) rf.commitIndex, rf.lastApplied = rf.logs[0].Index, rf.logs[0].Index for i := 0; i \u0026lt; len(peers); i++ { rf.matchIndex[i], rf.nextIndex[i] = 0, rf.getLastLogEntry().Index+1 if i == me { continue } rf.replicatorCond[i] = *sync.NewCond(\u0026amp;sync.Mutex{}) go rf.replicator(i) } // start ticker goroutine to start elections \tgo rf.ticker() go rf.applier(rf.applyCh) return rf } 根据论文，日志的index和term都从1开始，所以在logs[0]处存放一个index和term均为0的dummy value。\n在Make函数中启动了一些后台协程\n replicator：共len(peers)-1个，用于管理leader对每一个follower的日志复制，下文会详细介绍。 ticker：用来触发选举和心跳timeout。 applier：用于向applyCh中提交已经commit的日志。  leader-election sender 在ticker函数中需要循环使用select监听两个timer的channel，lab的提示中说使用timer可能会有问题但我没有遇到过，懒得改了。\n如果是选举计时器到期，则发起一轮选举；如果是心跳计时器到期，则发起一轮心跳。二者都要首先判断当前身份是否正确。我使用了一个map来记录当前term中投票给自己的peer，需要在每次转换为candidate时清空map。也可以每次start election时声明一个得票计数，之后使用闭包来计算。\nfunc (rf *Raft) ticker() { for !rf.Killed() { select { case \u0026lt;-rf.electionTimer.C: rf.rw.Lock() if rf.currentState != Leader { rf.currentState = Candidate rf.voteFrom = make(map[int]bool) rf.currentTerm++ rf.startElection() } rf.electionTimer.Reset(RandomizedElectionTimeout()) rf.rw.Unlock() case \u0026lt;-rf.heartbeatTimer.C: rf.rw.Lock() if rf.currentState == Leader { DPrintf(\u0026#34;[Server %d] boardcast heartbeat at term %d\u0026#34;, rf.me, rf.currentTerm) rf.boardcastHeartbeat(true) } rf.rw.Unlock() } } } 选举需要异步对每个peer发送request vote，不然就太慢了。异步才不会阻塞ticker，能快速重置计时器。response handler中要先判断是否仍满足rf.currentTerm == args.Term \u0026amp;\u0026amp; rf.currentState == Candidate，若不满足说明RPC过期，直接抛弃不处理。\n我之所以没有使用闭包是因为这样难以抽象出一个 handleRequestVoteResponse 函数，代码结构不够统一。\nfunc (rf *Raft) startElection() { args := rf.getDefaultRequestVoteArgs() rf.votedFor, rf.voteFrom[rf.me] = rf.me, true rf.persist() DPrintf(\u0026#34;[Server %d] start election at term %d\u0026#34;, rf.me, rf.currentTerm) for index := range rf.peers { if index == rf.me { continue } go func(i int) { reply := RequestVoteReply{} if rf.sendRequestVote(i, \u0026amp;args, \u0026amp;reply) { rf.rw.Lock() rf.handleRequestVoteResponse(i, \u0026amp;args, \u0026amp;reply) rf.rw.Unlock() } }(index) } } handler handler的实现完全参照论文，先判断term是否小于自身，再判断term、voteFor和日志是否满足条件。判断voteFor时要先满足args.Term == rf.currentTerm，这是由于args.Term \u0026gt; rf.currentTerm时需要无条件跟随term并重置voteFor。\n需要注意的是只有同意投票时才需要重置election timer，这在课程的TA的guidance中有提及，有利于在网络不稳定时仍能快速选出leader。\nfunc (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) { // Your code here (2A, 2B). \tdefer rf.rw.Unlock() defer DPrintf(\u0026#34;[Server %d] reply [%s] for RequestVote to %d\u0026#34;, rf.me, reply, args.CandidateId) rf.rw.Lock() DPrintf(\u0026#34;[Server %d][state %s term %d vote %d lastindex %d lastterm %d] receive RequestVote [%s] from %d\u0026#34;, rf.me, StateName[rf.currentState], rf.currentTerm, rf.votedFor, rf.getLastLogEntry().Index, rf.getLastLogEntry().Term, args, args.CandidateId) if args.Term \u0026lt; rf.currentTerm || (args.Term == rf.currentTerm \u0026amp;\u0026amp; rf.votedFor != -1 \u0026amp;\u0026amp; rf.votedFor != args.CandidateId) { reply.Term, reply.VoteGranted = rf.currentTerm, false return } needToPersist := false if args.Term \u0026gt; rf.currentTerm { rf.currentTerm, rf.votedFor = args.Term, -1 DPrintf(\u0026#34;[Server %d] change state from Leader to Follower at term %d\u0026#34;, rf.me, rf.currentTerm) rf.currentState = Follower needToPersist = true } if !rf.isLogUpToDate(args.LastLogIndex, args.LastLogTerm) { reply.Term, reply.VoteGranted = rf.currentTerm, false if needToPersist { rf.persist() } return } reply.Term, reply.VoteGranted = rf.currentTerm, true rf.votedFor = args.CandidateId rf.persist() rf.electionTimer.Reset(RandomizedElectionTimeout()) } log-replication replicator 根据每个peer的nextIndex判断发送entries或是snapshot。\nresponse handler的实现参照论文，先判断是否过期，再判断是否成功。若成功，则更新match、next index。找到最新的复制到超过半数peer且term等于当前term的日志，更新commit。需要注意日志的term必须和当前term一致才能更新commit，不然可能会有安全性问题导致已经commit的日志被覆盖，我忘了哪个测试一直过不了后来发现就是这个原因，所以论文一定要非常仔细读。\nfunc (rf *Raft) doReplicate(i int) { rf.rw.RLock() if rf.currentState != Leader { rf.rw.RUnlock() return } if rf.nextIndex[i] \u0026lt;= rf.getDummyLogntry().Index { args := rf.getDefaultInstallSnapshotArgs() rf.rw.RUnlock() reply := InstallSnapshotReply{} if rf.sendInstallSnapshot(i, \u0026amp;args, \u0026amp;reply) { rf.rw.Lock() rf.handleInstallSnapshotResponse(i, args, reply) rf.rw.Unlock() } } else { args := rf.getDefaultAppendEntriesArgs(i) rf.rw.RUnlock() reply := AppendEntriesReply{} if rf.sendAppendEntries(i, \u0026amp;args, \u0026amp;reply) { rf.rw.Lock() rf.handleAppendEntriesReponse(i, \u0026amp;args, \u0026amp;reply) rf.rw.Unlock() } } } 这里我参考了 LebronAI 的设计。\n如果为每一次Start、心跳都广播发送一次append entries，则将下层的日志同步与上层的提交新指令强绑定了，会造成RPC数量过多，还会重复发送很多次相同的日志项。每次发送 rpc 都不论是发送端还是接收端都需要若干次系统调用和内存拷贝，rpc 次数过多也会对 CPU 造成不必要的压力。\n这里可以做一个batching的优化，也将二者之间解耦。这里原作者参考了SOFAJraft的日志复制模型，让每个peer对于其他所有peer各维护一个replicator协程，负责在自己成为leader时对单独一个peer的日志复制。\n这个协程利用条件变量 sync.Cond 执行 Wait 来避免耗费 cpu，每次需要进行一次日志复制时调用 Signal 唤醒。它在满足复制条件时会尽最大努力将[nextIndex, lastIndex]之间的日志复制到peer上。\n由于leader使用replicator维护对于一个peer的日志复制，同一时间下最多只会发送一个RPC，若RPC丢失、超时很可能触发re-election。因此：\n 心跳计时器到期，很急，要立即发送RPC。leader commit更新时也要立即发送RPC，这个是为啥我忘记了。 Start被调用，不急，只需调用条件变量的 Singal，让replicator慢慢发。  func (rf *Raft) replicator(peer int) { rf.replicatorCond[peer].L.Lock() defer rf.replicatorCond[peer].L.Unlock() for !rf.Killed() { for !rf.needToReplicate(peer) { rf.replicatorCond[peer].Wait() } rf.doReplicate(peer) } } func (rf *Raft) needToReplicate(peer int) bool { rf.rw.RLock() defer rf.rw.RUnlock() return rf.currentState == Leader \u0026amp;\u0026amp; rf.nextIndex[peer] \u0026lt;= rf.getLastLogEntry().Index } func (rf *Raft) boardcastHeartbeat(isHeartbeat bool) { for index := range rf.peers { if index == rf.me { continue } if isHeartbeat { go rf.doReplicate(index) } else { rf.replicatorCond[index].Signal() } } rf.heartbeatTimer.Reset(StableHeartbeatTimeout()) } handler 完全按照论文图中伪代码实现，包括了课程视频中提到的加速解决日志冲突的优化。\nfunc (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) { rf.rw.Lock() DPrintf(\u0026#34;[Server %d][term %d lastindex %d lastterm %d commit %d] receive AppendEntries %+v from %d\u0026#34;, rf.me, rf.currentTerm, rf.getLastLogEntry().Index, rf.getLastLogEntry().Term, rf.commitIndex, args, args.LeaderId) defer rf.rw.Unlock() defer DPrintf(\u0026#34;[Server %d] reply [%s] for AppendEntries to %d\u0026#34;, rf.me, reply, args.LeaderId) needToPersist := false if args.Term \u0026lt; rf.currentTerm { reply.Success, reply.Term = false, rf.currentTerm return } if args.Term \u0026gt; rf.currentTerm { rf.currentTerm = args.Term needToPersist = true } rf.currentState = Follower rf.electionTimer.Reset(RandomizedElectionTimeout()) if args.PrevLogIndex \u0026lt; rf.getDummyLogntry().Index { reply.Success, reply.Term = false, 0 if needToPersist { rf.persist() } return } if !rf.isLogMatch(args.PrevLogIndex, args.PrevLogTerm) { reply.Term, reply.Success = rf.currentTerm, false reply.XIndex, reply.Term = rf.getConflictEntry(args.PrevLogIndex) if needToPersist { rf.persist() } return } lastLogIndex := rf.getLastLogEntry().Index for index, entry := range args.Entries { if entry.Index \u0026gt; lastLogIndex || rf.logs[rf.getSliceIndex(entry.Index)].Term != entry.Term { rf.logs = append(rf.logs[:rf.getSliceIndex(entry.Index)], args.Entries[index:]...) DPrintf(\u0026#34;[Server %d] append Follower\u0026#39;s last log index from %d to %d\u0026#34;, rf.me, lastLogIndex, rf.getLastLogEntry().Index) needToPersist = true break } } rf.maybeAdvanceFollowerCommit(args.LeaderCommit) reply.Success = true if needToPersist { rf.persist() } } persistence 论文中提到有三个变量是需要持久化的：currentTerm、votedFor、log[]，这三个量每次改变之后都应该持久化。\n持久化应当在被其他协程感知（发送RPC、释放锁）之前完成，而每个函数中如果没有改变这三个量（如加锁之后发现RPC过期）则不用持久化，若有也只需持久化一次，所以我在很多地方都使用了一个 needToPersist 布尔量进行判断。这样写感觉不够优雅，暂时没想到其他方法。\nlog-compaction 对于leader，在replicator中根据next index判断出需要给peer发送快照时，调用 persister.ReadSnapshot 获得快照并发送。\n对于接收方，需要判断如果 args.LastIncludedIndex \u0026lt;= rf.commitIndex，则拒绝接收快照。这说明本地状态机已经至少比该快照更新（或者将要，因为applier协程已经在apply这些日志的过程中了），可能导致raft回到旧的状态。应当等待service层调用 Snapshot 函数来安装快照。接收快照后，异步写入到applyCh中。\n对于两个service层给raft层安装快照的函数，它们的区别是：Snapshot 是由service层在处理apply message时判断raft state\u0026rsquo;s size是否达到阈值，主动调用。CondInstallSnapshot 是service层在处理apply message中leader发来的更新的快照时调用，也需要再次判断是否 LastIncludedIndex \u0026lt;= rf.commitIndex，安装快照之后应该更新lastApplied、commitIndex。\n安装快照后需要压缩日志，但是需要记录下包含在快照中的最新的日志项的index和term，我将其记录在dummy entry（即rf.log[0]）中。此外被删除的日志项需要被正确的删除使其能够被gc。\napplier 根据论文，一旦commitIndex \u0026gt; lastApplied，则需要将[lastApplied+1, commitIndex]中的所有日志项apply到状态机并增加lastApplied。\n一开始我的实现是每次commitIndex更新，都异步起一个协程将[lastApplied+1, commitIndex]间日志写入applyCh。但是因为写channel时不能持有锁，所以这个过程只能是：\n加锁 -\u0026gt; 深拷贝日志项 -\u0026gt; 释放锁 -\u0026gt; 写channel -\u0026gt; 加锁 -\u0026gt; 更新lastApplied -\u0026gt; 释放锁\n日志在push完之前不会更新lastApplied，这样容易造成相同的日志项被重复apply，存在资源浪费。所以这里也可以参考之前replicator的实现思路，后台起一个applier协程，平时调用一个条件变量的 Wait ，被 Signal 唤醒时将[lastApplied+1, commitIndex]中的所有日志项apply到状态机，每次更新commitIndex时调用 Signal。这样即能避免日志被重复apply，也完成了 apply 日志到状态机和 raft 提交新日志之间的解耦。\nfunc (rf *Raft) applier(applyCh chan ApplyMsg) { for !rf.Killed() { rf.rw.Lock() for !rf.needToApply() { rf.applierCond.Wait() } lastApplied, commitIndex := rf.lastApplied, rf.commitIndex needToApply := DeepCopy(rf.logs[rf.getSliceIndex(lastApplied+1) : rf.getSliceIndex(commitIndex)+1]) rf.rw.Unlock() for _, entry := range needToApply { applyMsg := ApplyMsg{ CommandValid: true, Command: entry.Command, CommandIndex: entry.Index, CommandTerm: entry.Term, RaftStateSize: rf.persister.RaftStateSize(), } applyCh \u0026lt;- applyMsg DPrintf(\u0026#34;[Server %d] applied log [index %d] at term %d\u0026#34;, rf.me, entry.Index, entry.Term) } rf.rw.Lock() if commitIndex \u0026gt; rf.lastApplied { rf.lastApplied = commitIndex } rf.rw.Unlock() } } 需要注意因为写channel时是不加锁的，而写channel是可能出现并发的，可能存在一种情况：applier在写入一批旧日志时，follower接受leader的 InstallSnapshot 之后将新的snapshot写入channel。此时channel的写入顺序可能是：旧日志1 -\u0026gt; 新快照 -\u0026gt; 旧日志2。\nservice层读channel是线性的，在读出snapshot并调用 CondInstallSnapshot 后会更新raft层的lastApplied、commitIndex。因此在raft层apply完日志之后，重新获得锁去更新lastApplied时要注意不能回退，在这二者之间可能service层已经对更新的快照调用过 CondInstallSnapshot 了（新快照的 lastIncludeIndex 一定大于 commitIndex ）。\n","date":"2022-02-03T15:04:30+08:00","permalink":"https://cza2000.github.io/2022/mit-6.824-spring-2021-lab2-raft/","title":"mit-6.824 Spring 2021 lab2: Raft"},{"content":"Lab1: MapReduce 在本次lab中我们的任务是实现一个分布式的MapReduce，它由两个程序组成，Coordinator和Worker。只有一个Coordinator，一个或多个Worker并行执行。\n每个Worker将通过RPC与Coordinator通信以请求一个Map或Reduce任务，之后从一个或多个文件中读取任务的输入，执行任务，并将任务的输出写入一个或多个文件。\nCoordinator应注意到Worker是否在合理的时间内（10s）完成了任务，如果没有则将相同的任务交给另一个Worker。\nCoordinator 写这个lab的时候刚学go语言不久，觉得channel这个东西很帅，就使用了很多channel实现了一个lock-free版本的Coordinator，实践了一下csp。\n核心结构体 Coordinator维护每一个Map和Reduce任务的状态，这样就不用维护每一个worker的状态，这也利于worker的弹性伸缩。\nxxxidCh用于在获取任务编号并发放给worker，xxxDoneCh和xxxUndoneCh用于获取完成或未完成的任务编号修改任务状态。\ntype Coordinator struct { files []string nMap int nReduce int mapidCh chan int reduceidCh chan int mapStatus []Task reduceStatus []Task heartbeatCh chan heartbeatMsg reportCh chan reportMsg stateCh chan getStateMsg mapDoneCh chan Execution reduceDoneCh chan Execution mapUndoneCh chan Execution reduceUndoneCh chan Execution mapComplete bool reduceComplete bool mapRemain int reduceRemain int } 每个任务的状态有3种，每个任务被初始化时都是UnStarted，被分配给Worker之后转换为Processing，收到Report完成转为Done，未完成转为UnStarted。\n结构体Task用term和任务状态共同表示一个任务的信息，term代表该任务被分配给worker执行的次数。\ntype TaskStatus int const ( UnStarted TaskStatus = iota Processing Done ) type Task struct { term int TaskStatus } RPC-handler Coordinator接收到RPC之后，包装出一个xxxMsg结构，传入RPC对应的channel中。\nDone在这里作用类似于一个回调。Coordinator在启动时会在后台启动一个goroutine，不断监控 heartbeatCh 和 reportCh 中的Msg并处理，处理完成后执行msg.Done \u0026lt;- struct{}{}。在RPC handler中只需要等待Done这个channel返回。\ntype heartbeatMsg struct { response *HeartbeatResponse Done chan struct{} } type reportMsg struct { request *ReportRequest Done chan struct{} } func (c *Coordinator) Heartbeat(request *HeartbeatRequest, response *HeartbeatResponse) error { log.Println(\u0026#34;[Coordinator] receive a request from worker\u0026#34;) msg := heartbeatMsg{ response: response, Done: make(chan struct{}), } c.heartbeatCh \u0026lt;- msg \u0026lt;-msg.Done log.Printf(\u0026#34;[Coordinator] run heartbeat [%s] for worker\u0026#34;, response) return nil } func (c *Coordinator) Report(request *ReportRequest, response *ReportResponse) error { log.Printf(\u0026#34;[Coordinator] receive worker\u0026#39;s report [%s]\u0026#34;, request) msg := reportMsg{ request: request, Done: make(chan struct{}), } c.reportCh \u0026lt;- msg \u0026lt;-msg.Done log.Println(\u0026#34;[Coordinator] finish dealing with the report from worker\u0026#34;) return nil } handleHeartbeatMsg函数中处理心跳，根据当前Map和Reduce任务的状态给Worker分配一个任务、让worker等待或是告知所有任务已经完成。任务的id从mapidCh或reduceidCh两个channel中读出，在response中还要加上任务的term，每次分配该任务前需要对term自增以在不同的执行者之间区分。\n那么任务的id是什么时候写入channel中的呢？Coordinator在初始化时先将所有Map任务的id写入mapidCh，在所有Map任务都完成后将所有Reduce任务的id写入reduceidCh。\n需要注意一点，每个任务在分配之后10s内如果没有收到Report，则应该默认任务失败。这需要另起一个goroutine来判断，直接sleep 10s之后将id写入Undone channel即可，让run函数去判断。\n在handleReportMsg函数中处理worker的返回任务结果，根据结构类型将任务的Execution写入对应的Done/Undone channel。我将任务的term和id包装成一个Execution结构表示任务的一次执行，使得某次任务失败是超时还是worker返回失败这两种情况可以被区分。\ntype Execution struct { term int id int } 核心逻辑 run函数是Coordinator的核心，它作为一个后台运行的goroutine在不断的循环中监听各个channel并执行对应的操作。由于所有的数据都在这一个goroutine中修改，避免了data-race。\nCoordinator真正处理worker上报的任务的完成情况是由run函数在select中同时监听这4个channel，再根据任务id来执行对应逻辑。因此handleReportMsg函数可以另起一个goroutine来执行，这4个channel的容量也只需设置为1。\n从4个channel读出任务id后要注意，只有在对应的状态、Execution中term和本地任务的term一致时才能执行逻辑。\n例如某个MapFailed消息在10s之后到达，这可能是因为网络拥塞或是worker执行任务太慢，这个map任务已经被重新分配给了另一个worker，此时状态是仍是Processing。但这时term不一致应该放弃处理这个MapFailed消息。\nfunc (c *Coordinator) run() error { for { select { case hbMsg := \u0026lt;-c.heartbeatCh: c.handleHeartbeatMsg(hbMsg) case rpMsg := \u0026lt;-c.reportCh: go c.handleReportMsg(rpMsg) case e := \u0026lt;-c.mapDoneCh: if c.mapStatus[e.id].TaskStatus == Processing \u0026amp;\u0026amp; c.mapStatus[e.id].term == e.term { ··· } case e := \u0026lt;-c.reduceDoneCh: if c.reduceStatus[e.id].TaskStatus == Processing \u0026amp;\u0026amp; c.reduceStatus[e.id].term == e.term { ··· } case e := \u0026lt;-c.mapUndoneCh: if c.mapStatus[e.id].TaskStatus == Processing \u0026amp;\u0026amp; c.mapStatus[e.id].term == e.term { ··· } case e := \u0026lt;-c.reduceUndoneCh: if c.reduceStatus[e.id].TaskStatus == Processing \u0026amp;\u0026amp; c.reduceStatus[e.id].term == e.term { ··· } case stMsg := \u0026lt;-c.stateCh: stMsg.state \u0026lt;- c.reduceComplete } } } Worker worker的实现比较简单，只需要循环向coordinator请求任务执行。\nfunc Worker(mapf func(string, string) []KeyValue, reducef func(string, []string) string) { // Your worker implementation here. \tfor { response := doHeartbeat() log.Printf(\u0026#34;[Worker] receive coordinator\u0026#39;s heartbeat [%s]\u0026#34;, response) switch response.Type { case Map: doMapTask(mapf, response.Id, response.Term, response.NReduce, response.Name) case Reduce: doReduceTask(reducef, response.Id,response.Term, response.NMap) case Wait: time.Sleep(1 * time.Second) case Completed: return default: panic(fmt.Sprintf(\u0026#34;[Worker] unexpected jobType %v\u0026#34;, response.Type)) } } } 执行Map任务时，只需将mapf函数产生的中间文件kv pair按照ihash(kv.Key)%nReduce的余数写入不同的文件等待Reduce即可。写入的文件要先调用ioutil.TempFile(\u0026quot;\u0026quot;, \u0026ldquo;temp\u0026rdquo;)生成再调用os.Rename()改为mr-i-j。\n执行Reduce任务时，先建立一个kv数组，再将所有中间文件中的kv pair append到数组中再排序，将相同key对应的所有value append到一个string数组中，喂给reducef函数执行。看起来非常暴力，在工业界应该不可行，但通过本次lab的测试足够了。\n","date":"2022-02-03T15:01:24+08:00","permalink":"https://cza2000.github.io/2022/mit-6.824-spring-2021-lab1-mapreduce/","title":"mit-6.824 Spring 2021 lab1: MapReduce"},{"content":"hello world1 777\nhello world2 777\nHello Hugo3  aaa bbb ccc  func main() { fmt.Printf(\u0026#34;Hello world\u0026#34;) } hello world2 777\nhello world2 777\nHello Hugo3 hello world2 888\n","date":"2022-01-31T14:38:17+08:00","permalink":"https://cza2000.github.io/2022/test/","title":"Test"}]